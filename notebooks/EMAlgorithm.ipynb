{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modules for Loading Data\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import os, numpy, PIL\n",
    "from PIL import Image\n",
    "import matplotlib .pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "'''\n",
    "Defining the psuedo code for the EM algorithm for a mixture of gaussian Models\n",
    "The parameters unknown to us - \n",
    "1)the number of models for the given data,\n",
    "2)the parameters of each of the models\n",
    "3)the number of iterations\n",
    "\n",
    "Consider the case where we have to fit 'n' Gaussian Models on a given dataset\n",
    "The idea is to fit 'n' Gaussian models on a given data\n",
    "\n",
    "Input: Training data , number of clusters\n",
    "Output: Estimates of parameters, theta\n",
    "'''\n",
    "\n",
    "#Declaring all the global variables\n",
    "pca = scaler = lold = 0\n",
    "'''\n",
    "This function will do all the global operations on the data, 'scaler' and 'pca' should be global variables\n",
    "'''\n",
    "\n",
    "def dataOperations(data): # Make sure this RUNS after the data loading is done, so that scaler and pca objects are available\n",
    "    #Calculating PCA, reducing the dimensionality\n",
    "    global scaler\n",
    "    global pca\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "\n",
    "    Xfeatures = scaler.transform(data)\n",
    "    #print(Xfeatures)\n",
    "    #print(Xfeatures.shape)\n",
    "    #print(Xfeatures)\n",
    "        \n",
    "        \n",
    "    pca = PCA(n_components=50)\n",
    "    #print(pca.explained_variance_ratio)\n",
    "    Xnew = pca.fit_transform(data)\n",
    "    return Xnew # returns the pca of the data\n",
    "\n",
    "def loadData(flag):\n",
    "        train_pos_path = '../dataset/train_pos/'\n",
    "        train_neg_path = '../dataset/train_neg/'\n",
    "        \n",
    "        X1 = np.array([np.zeros(10800)]) #This is the feature vector for all positive images\n",
    "        X0 = np.array([np.zeros(10800)]) #This is the feature vector for all negative images\n",
    "\n",
    "\n",
    "        #First finding u1, c1 for all the positives\n",
    "        for filename in os.listdir(train_pos_path):\n",
    "            imgpath = train_pos_path + filename\n",
    "            img = cv2.imread(imgpath,-1)\n",
    "            #print(img.shape)\n",
    "            x = np.asarray(img)\n",
    "            x = np.reshape(x,(1,10800))\n",
    "            #print(x.shape)\n",
    "            X1 = np.dstack([X1,x]) \n",
    "        #print(X1.shape)\n",
    "\n",
    "        for filename in os.listdir(train_neg_path):\n",
    "            imgpath = train_neg_path + filename\n",
    "            img = cv2.imread(imgpath,-1)\n",
    "            #print(img.shape)\n",
    "            x = np.asarray(img)\n",
    "            x = np.reshape(x,(1,10800))\n",
    "            #print(x.shape)\n",
    "            X0 = np.dstack([X0,x])\n",
    "        #print(X0.shape)\n",
    "\n",
    "        Y1 = np.array([np.ones(1000)])\n",
    "        #print(Y1.shape)\n",
    "\n",
    "        Y0 = np.array([np.zeros(1000)])\n",
    "        #print(Y0.shape)\n",
    "        \n",
    "        X1 = np.delete(X1, 0, axis=2)\n",
    "        X1 = np.reshape(X1,(1000,10800))\n",
    "        #print(X1.shape)\n",
    "        X0 = np.delete(X0, 0, axis=2)\n",
    "        X0 = np.reshape(X0,(1000,10800))\n",
    "        #print(X0.shape)\n",
    "        \n",
    "        if flag == 1:\n",
    "            return X1\n",
    "        else:\n",
    "            return X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10800)\n",
      "(1000, 50)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'ExpectationMaximisation' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-66ab7cffacad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mParameters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mWill\u001b[0m \u001b[0mspecify\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mGaussian\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m ''' \n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mExpectationMaximisation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-66ab7cffacad>\u001b[0m in \u001b[0;36mExpectationMaximisation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-66ab7cffacad>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Creating EM object with the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# Header parameters - maxiters,clusters,data,parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExpectationMaximisation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataNew\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;31m#print(type(em))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'ExpectationMaximisation' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "##  Defining the EM module\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import os, PIL\n",
    "from PIL import Image\n",
    "import matplotlib .pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "dictor=0\n",
    "'''\n",
    "    Iterations - Max number of iterations the model is going to run till\n",
    "    Clusters - Max number of clusters\n",
    "    Data - Will be the feature vector of the dataset (class-specific) -- this should be SCALED\n",
    "    Parameters - Will specify the number of parameters ( Gaussian - 2)\n",
    "''' \n",
    "class ExpectationMaximisation:   \n",
    "    \n",
    "    def __init__(self,m,c,d,n):\n",
    "        self.maxiters = m\n",
    "        self.clusters = c\n",
    "        self.data = d\n",
    "        self.nparameters = n\n",
    "    '''\n",
    "    For defining the EM algorithm\n",
    "    '''\n",
    "    def eMaximisation(self):\n",
    "        global lold;\n",
    "        cthreshold = 0.1\n",
    "        iters = 0\n",
    "        lold = 0\n",
    "        lnew = L = 1\n",
    "        \n",
    "        while((lnew-lold) > cthreshold and iters <= self.maxiters):\n",
    "            if(iters==0):\n",
    "                lold = 0\n",
    "            else:\n",
    "                lold = L\n",
    "            iters = iters+1\n",
    "            x = self.data.shape[0]\n",
    "            y = self.data.shape[1]\n",
    "\n",
    "            likelihood = np.zeros((x,self.clusters))\n",
    "            r = np.zeros((x,self.clusters))\n",
    "            lnew = np.zeros(self.clusters)\n",
    "            unew = np.zeros((self.clusters,x))\n",
    "            cnew = np.zeros((self.clusters,x*x))\n",
    "\n",
    "            model = np.zeros(self.clusters)\n",
    "\n",
    "            for i in range(x):\n",
    "                #every data element is data[i]\n",
    "                for k in range(self.clusters):\n",
    "                    #fit the model with the u(k) and sigma(k)\n",
    "                    model[k] = multivariate_normal(dictor[i][0],dictor[i][1]) # the [0] is mean and the [1] is covariance\n",
    "\n",
    "                    d = inputTransform(self.data[i,:])\n",
    "                    likelihood[i,k] = l[k] * model[k].pdf(d)\n",
    "                #for ends\n",
    "\n",
    "                for k in range(self.clusters):\n",
    "                    sumk = np.sum(likelihood,axis=1)[k]\n",
    "                    r[i,k] = likelihood[i,k]/sumk\n",
    "                #for ends\n",
    "            #for ends\n",
    "\n",
    "            ## The M step starts here\n",
    "            for k in range(self.clusters):\n",
    "                lnew[k] = np.sum(r[:,k],axis=0)/np.sum(r) #this is the new prior\n",
    "                unew[k] = np.sum(r[:,k]*self.data[i,:],axis=0)/np.sum(r[:,k],axis=0) #this is the updated mean\n",
    "                cnew[k] = np.sum(r[:,k]*(self.data-unew[k]) * np.transpose(self.data-unew[k]),axis=0)/np.sum(r[:,k],axis=0) # this is the updates covariance\n",
    "            #for ends\n",
    "\n",
    "            #find the data log likelihood\n",
    "            for i in range(x):\n",
    "                for k in range(self.clusters):\n",
    "                    model[k] = multivariate_normal(dictor[i][0],dictor[i][1]) # the [0] is mean and the [1] is covariance\n",
    "                    d = inputTransform(self.data[i,:])\n",
    "                    prob = l[k]*model[k].pdf(d)\n",
    "                    p = np.sum(prob)\n",
    "                lg = np.log(p)\n",
    "            L = np.sum(lg) #finding the log-likelihood\n",
    "            lnew = L\n",
    "            \n",
    "        a = [model,l]\n",
    "        return a\n",
    "\n",
    "    '''\n",
    "    Calculates the transform of the data using pca and scaled\n",
    "    '''   \n",
    "    def inputTransform(self,x):\n",
    "        x = np.reshape(x,(1,10800))\n",
    "        scaled = scaler.transform(x)\n",
    "        xtest = pca.transform(scaled)\n",
    "        return xtest\n",
    "    '''\n",
    "    Finding the covariance of the data\n",
    "    '''     \n",
    "        \n",
    "    def findCov(self,data):\n",
    "        x = data.shape[0]\n",
    "        y = data.shape[1]\n",
    "        covariance = np.cov(data.reshape(y,x))\n",
    "        return covariance\n",
    "    \n",
    "    '''\n",
    "    For initiliasing all the variables which will be used for calculating E- step and M-step\n",
    "    '''\n",
    "    def initializationEM(self,data):\n",
    "        x = data.shape[0]\n",
    "        y = data.shape[1]\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "        \n",
    "        l = np.ones(self.clusters)/self.clusters #setting the prior initially to 1/classes\n",
    "        \n",
    "        mean= np.zeros((self.clusters,y)) # initilaises the mean to a random row in the dataset\n",
    "        #np.random.seed(2)\n",
    "        #idx=np.random.randint(x,size=1)\n",
    "        \n",
    "        for i in range(self.clusters):\n",
    "            idx = np.random.randint(x,size=1)\n",
    "            mean[i,:] = data[idx,:]\n",
    "        \n",
    "        cov = self.findCov(data) #this finds the covariance of the data\n",
    "        \n",
    "        #Initialising the parameters\n",
    "        #The parameters will be mean and covariance\n",
    "        \n",
    "        pdict = {}\n",
    "        for i in range(self.clusters):\n",
    "            pdict[i] = [mean[i,:],cov]\n",
    "        return pdict\n",
    "    '''\n",
    "    Loads data into feature vectors\n",
    "    '''        \n",
    "    \n",
    "    def main():\n",
    "        #global ExpectationMaximisation\n",
    "        global dicto\n",
    "        #data loading\n",
    "        data1 = loadData(1)\n",
    "        print(data1.shape)\n",
    "        \n",
    "        #dataoperations\n",
    "        dataNew = dataOperations(data1)\n",
    "        print(dataNew.shape)\n",
    "        \n",
    "        # Creating EM object with the parameters\n",
    "        # Header parameters - maxiters,clusters,data,parameters\n",
    "        em = ExpectationMaximisation(500,2,dataNew,2)\n",
    "        #print(type(em))\n",
    "        \n",
    "        #initialisation\n",
    "        dictor = em.initializationEM(dataNew)\n",
    "        #print(dicto)\n",
    "        \n",
    "        #while the EM does not converge\n",
    "        result = em.eMaximisation()\n",
    "        print(\"Model and the corresponding parameters\",result)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dicto[0].value[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
